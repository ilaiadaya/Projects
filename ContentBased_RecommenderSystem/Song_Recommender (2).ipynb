{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based Recomendation System, tested on different word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>track_album_id</th>\n",
       "      <th>track_album_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0017A6SJgTbfQVU2EtsPNo</td>\n",
       "      <td>Pangarap</td>\n",
       "      <td>Barbie's Cradle</td>\n",
       "      <td>Minsan pa Nang ako'y napalingon Hindi ko alam ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1srJQ0njEQgd8w4XSqI4JQ</td>\n",
       "      <td>Trip</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Pinoy Classic Rock</td>\n",
       "      <td>37i9dQZF1DWYDQ8wBxd7xt</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.27900</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.566</td>\n",
       "      <td>97.091</td>\n",
       "      <td>235440</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>004s3t0ONYlzxII9PLgU6z</td>\n",
       "      <td>I Feel Alive</td>\n",
       "      <td>Steady Rollin</td>\n",
       "      <td>The trees, are singing in the wind The sky blu...</td>\n",
       "      <td>28</td>\n",
       "      <td>3z04Lb9Dsilqw68SHt6jLB</td>\n",
       "      <td>Love &amp; Loss</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>Hard Rock Workout</td>\n",
       "      <td>3YouF0u7waJnolytf9JCXf</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>00chLpzhgVjxs1zKC9UScL</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>6oZ6brjB8x3GoeSYdwJdPc</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Back in the day - R&amp;B, New Jack Swing, Swingbe...</td>\n",
       "      <td>3a9y4eeCJRmG9p4YKfqYIx</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.00723</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.650</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>00cqd6ZsSkLZqGMlQCR0Zo</td>\n",
       "      <td>Baby It's Cold Outside (feat. Christina Aguilera)</td>\n",
       "      <td>CeeLo Green</td>\n",
       "      <td>I really can't stay Baby it's cold outside I'v...</td>\n",
       "      <td>41</td>\n",
       "      <td>3ssspRe42CXkhPxdc12xcp</td>\n",
       "      <td>CeeLo's Magic Moment</td>\n",
       "      <td>2012-10-29</td>\n",
       "      <td>Christmas Soul</td>\n",
       "      <td>6FZYc2BvF7tColxO8PBShV</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.68900</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>00emjlCv9azBN0fzuuyLqy</td>\n",
       "      <td>Dumb Litty</td>\n",
       "      <td>KARD</td>\n",
       "      <td>Get up out of my business You don't keep me fr...</td>\n",
       "      <td>65</td>\n",
       "      <td>7h5X3xhh3peIK9Y0qI5hbK</td>\n",
       "      <td>KARD 2nd Digital Single ‘Dumb Litty’</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>K-Party Dance Mix</td>\n",
       "      <td>37i9dQZF1DX4RDXswvP6Mj</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.240</td>\n",
       "      <td>130.018</td>\n",
       "      <td>193160</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id                                         track_name  \\\n",
       "0  0017A6SJgTbfQVU2EtsPNo                                           Pangarap   \n",
       "1  004s3t0ONYlzxII9PLgU6z                                       I Feel Alive   \n",
       "2  00chLpzhgVjxs1zKC9UScL                                             Poison   \n",
       "3  00cqd6ZsSkLZqGMlQCR0Zo  Baby It's Cold Outside (feat. Christina Aguilera)   \n",
       "4  00emjlCv9azBN0fzuuyLqy                                         Dumb Litty   \n",
       "\n",
       "      track_artist                                             lyrics  \\\n",
       "0  Barbie's Cradle  Minsan pa Nang ako'y napalingon Hindi ko alam ...   \n",
       "1    Steady Rollin  The trees, are singing in the wind The sky blu...   \n",
       "2   Bell Biv DeVoe  NA Yeah, Spyderman and Freeze in full effect U...   \n",
       "3      CeeLo Green  I really can't stay Baby it's cold outside I'v...   \n",
       "4             KARD  Get up out of my business You don't keep me fr...   \n",
       "\n",
       "   track_popularity          track_album_id  \\\n",
       "0                41  1srJQ0njEQgd8w4XSqI4JQ   \n",
       "1                28  3z04Lb9Dsilqw68SHt6jLB   \n",
       "2                 0  6oZ6brjB8x3GoeSYdwJdPc   \n",
       "3                41  3ssspRe42CXkhPxdc12xcp   \n",
       "4                65  7h5X3xhh3peIK9Y0qI5hbK   \n",
       "\n",
       "                       track_album_name track_album_release_date  \\\n",
       "0                                  Trip               2001-01-01   \n",
       "1                           Love & Loss               2017-11-21   \n",
       "2                                  Gold               2005-01-01   \n",
       "3                  CeeLo's Magic Moment               2012-10-29   \n",
       "4  KARD 2nd Digital Single ‘Dumb Litty’               2019-09-22   \n",
       "\n",
       "                                       playlist_name             playlist_id  \\\n",
       "0                                 Pinoy Classic Rock  37i9dQZF1DWYDQ8wBxd7xt   \n",
       "1                                  Hard Rock Workout  3YouF0u7waJnolytf9JCXf   \n",
       "2  Back in the day - R&B, New Jack Swing, Swingbe...  3a9y4eeCJRmG9p4YKfqYIx   \n",
       "3                                     Christmas Soul  6FZYc2BvF7tColxO8PBShV   \n",
       "4                                  K-Party Dance Mix  37i9dQZF1DX4RDXswvP6Mj   \n",
       "\n",
       "   ... loudness mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "0  ...  -10.068    1       0.0236       0.27900           0.01170    0.0887   \n",
       "1  ...   -4.739    1       0.0442       0.01170           0.00994    0.3470   \n",
       "2  ...   -7.504    0       0.2160       0.00432           0.00723    0.4890   \n",
       "3  ...   -5.819    0       0.0341       0.68900           0.00000    0.0664   \n",
       "4  ...   -1.993    1       0.0409       0.03700           0.00000    0.1380   \n",
       "\n",
       "   valence    tempo  duration_ms  language  \n",
       "0    0.566   97.091       235440        tl  \n",
       "1    0.404  135.225       373512        en  \n",
       "2    0.650  111.904       262467        en  \n",
       "3    0.405  118.593       243067        en  \n",
       "4    0.240  130.018       193160        en  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "import numpy as np \n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score\n",
    "import json \n",
    "import random\n",
    "\n",
    "songs = pd.read_csv('spotify_songs.csv')\n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing non english and duplicates\n",
    "songs = songs.loc[songs['language'] == \"en\"]\n",
    "songs = songs.drop_duplicates(subset=['lyrics'])\n",
    "\n",
    "\n",
    "## isolating lyrics \n",
    "lyrics = list(songs[\"lyrics\"])\n",
    "track_names = list(songs[\"track_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-771-db48c503c3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m## lemmatize only useful words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0msong_clean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     23\u001b[0m         return [\n\u001b[1;32m     24\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         ]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## initializing lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "lyrics_clean = []\n",
    "i = 0\n",
    "\n",
    "## cleaning lyrics of songs \n",
    "for song in lyrics:\n",
    "    temp = re.sub('[^a-zA-Z]', ' ', song)\n",
    "    temp = temp.lower()\n",
    "    temp = temp.split(\" \")\n",
    "    song_clean = \"\"\n",
    "    for word in temp:\n",
    "        ## lemmatize only useful words\n",
    "        if len(word) > 3 and not word in set(stopwords.words('english')):\n",
    "            song_clean += wl.lemmatize(word) + \" \"\n",
    "            \n",
    "    ## list that stores all preprocessed lyrics \n",
    "    lyrics_clean.append(song_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tfidf vector\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "tfidf_sparse_matrix = tfidf_vectorizer.fit_transform(lyrics_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reducing the dimensions \n",
    "svd_dimensions = 100\n",
    "svd = TruncatedSVD(n_components= svd_dimensions, random_state=42)\n",
    "reduced_tfidf = svd.fit_transform(tfidf_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict = {}\n",
    "## creating a dict with track name being key and its respective vector being the value \n",
    "for i in range(len(lyrics_clean)):\n",
    "    tfidf_dict[songs.iloc[i][\"track_name\"]] = reduced_tfidf[i].reshape(1, svd_dimensions)\n",
    "    if songs.iloc[i][\"track_name\"] == \"Hola Beba\":\n",
    "        print(\"here\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using Doc2Vec to represent lyrics in document vectors \n",
    "vec_size = 100\n",
    "tagged_lyrics = [TaggedDocument(lyrics.split(), [i]) for i, lyrics in enumerate(lyrics_clean)]\n",
    "doc2vec_matrix = Doc2Vec(tagged_lyrics, vector_size= vec_size, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_dict = {}\n",
    "## creating a dict with track name being key and its respective vector being the value \n",
    "for i in range(len(lyrics_clean)):\n",
    "    doc2vec_dict[songs.iloc[i][\"track_name\"]] = doc2vec_matrix[i].reshape(1, vec_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert pretrained model Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/marcoliveau/.cache/torch/sentence_transformers/sbert.net_models_bert-base-nli-mean-tokens/0_BERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "## pretrained BERT model that creates sentence embeddings much faster than classical BERT models\n",
    "small_bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "document_embeddings = small_bert_model.encode(lyrics_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_dict = {}\n",
    "## creating a dict with track name being key and its respective vector being the value \n",
    "for i in range(len(lyrics_clean)):\n",
    "    sbert_dict[songs.iloc[i][\"track_name\"]] = document_embeddings[i].reshape(1, sbert_reshaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_dict[songs.iloc[0][\"track_name\"]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongRecommender:\n",
    "    def __init__(self, playlist, embeddings_dict):\n",
    "        self.playlist = playlist\n",
    "        self.embeddings_dict = embeddings_dict\n",
    "        \n",
    "    def finding_avg_vector(self):\n",
    "        vec_list = []\n",
    "        ## extracting all vectors\n",
    "        for song in self.playlist:\n",
    "            vec_list.append(self.embeddings_dict[song])\n",
    "        ## adding all vectors to a np array\n",
    "        main_array = vec_list[0]\n",
    "        for vec in vec_list[1:]:\n",
    "            np.concatenate((main_array, vec), axis = 0)\n",
    "        ## finding mean \n",
    "        avg_vec = np.array(main_array.mean(0))\n",
    "        return avg_vec\n",
    "\n",
    "    def song_weeder(self, recommended_songs):\n",
    "        final_recommendation = []\n",
    "        ## collecting only the songs that are NOT in the OG playlist \n",
    "        for song in recommended_songs:\n",
    "            if song[0] not in playlist:\n",
    "                final_recommendation.append([song[0], song[1], song[2]])\n",
    "        return final_recommendation\n",
    "\n",
    "    def n_most_similar(self, user_vec, n):\n",
    "        similarity_scores = []\n",
    "        ##finding all similarities \n",
    "        for i in range(len(self.embeddings_dict)):\n",
    "            similarity_scores.append((i, float(cosine_similarity([user_vec], list(self.embeddings_dict.values())[i]))))\n",
    "        ## sorting scores in descending order, with its respective position \n",
    "        sorted_scores = sorted(similarity_scores, key=lambda tup: tup[1], reverse= True)\n",
    "        recommended_songs = []\n",
    "        ## collecting names of songs with highest similarity scores, with its respective score \n",
    "        for pos, score in sorted_scores[:n*2]:\n",
    "            recommended_songs.append([songs.iloc[pos][\"track_name\"], songs.iloc[pos][\"track_artist\"], score])\n",
    "        ## removing any recommended songs that already exist in the playlist\n",
    "        final_recommendation = self.song_weeder(recommended_songs)\n",
    "        return final_recommendation[:n]\n",
    "\n",
    "    def print_recommended(self, recommended_songs, n):\n",
    "        i = 1\n",
    "        ## printing message for each recommended song \n",
    "        for song in recommended_songs[:n]:\n",
    "            print(\"Song recommended number\", i,  \": \"+ song[0], \"by -\" , song[1], \"with a similarity score of \", song[2])\n",
    "            print(\"--------------------------------\")\n",
    "            i +=1\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender(playlist, embedding_dict, embedding_name):\n",
    "    print(\"WORD EMBEDDING IN USE:\", embedding_name)\n",
    "    print(\"-----------------------\")\n",
    "    print(\"PLAYLIST:\", playlist)\n",
    "    print(\"-----------------------\")\n",
    "    sr = SongRecommender(playlist, embedding_dict)\n",
    "    user_vec = sr.finding_avg_vector()\n",
    "    n = round(len(playlist) * 0.4)\n",
    "    recommended_songs = sr.n_most_similar(user_vec, n)\n",
    "    sr.print_recommended(recommended_songs, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Recommender with different Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating random playlist for testing \n",
    "i= 123\n",
    "j= 456\n",
    "k= 789\n",
    "l = 987\n",
    "m = 654\n",
    "n = 321\n",
    "playlist = [songs.iloc[i][\"track_name\"], songs.iloc[j][\"track_name\"], songs.iloc[k][\"track_name\"], \n",
    "            songs.iloc[l][\"track_name\"], songs.iloc[m][\"track_name\"], songs.iloc[n][\"track_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender(playlist, tfidf_dict, \"TFIDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD EMBEDDING IN USE: DOC2VEC\n",
      "-----------------------\n",
      "PLAYLIST: ['April Showers', 'I Lived - Arty Remix', 'We Start Fires', 'You Are My Heart', 'Anywhere', 'Stairway To Heaven']\n",
      "-----------------------\n",
      "Song recommended number 1 : Afterlife by - Greyson Chance with a similarity score of  0.6775115728378296\n",
      "--------------------------------\n",
      "Song recommended number 2 : Somebody For Me by - Heavy D & The Boyz with a similarity score of  0.6732325553894043\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "recommender(playlist, doc2vec_dict, \"DOC2VEC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD EMBEDDING IN USE: SBERT\n",
      "-----------------------\n",
      "PLAYLIST: ['April Showers', 'I Lived - Arty Remix', 'We Start Fires', 'You Are My Heart', 'Anywhere', 'Stairway To Heaven']\n",
      "-----------------------\n",
      "Song recommended number 1 : Afterlife by - Greyson Chance with a similarity score of  0.7414441704750061\n",
      "--------------------------------\n",
      "Song recommended number 2 : DJ Turn It Up by - Yellow Claw with a similarity score of  0.7410348057746887\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "recommender(playlist, sbert_dict, \"SBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading data from previous notebooks output\n",
    "a_file = open(\"data.json\", \"r\")\n",
    "output = a_file.read()\n",
    "res = json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating new playlists for evaluation, only containing usable songs and playlists of length greater than 10\n",
    "count = 0\n",
    "eval_dict = {}\n",
    "for playlist, song_list in res.items():\n",
    "    clean_songs = []\n",
    "    for song in song_list:\n",
    "        if song in track_names:\n",
    "            clean_songs.append(song)\n",
    "    if len(clean_songs) > 9:\n",
    "        splitter = len(clean_songs) - round(len(clean_songs) * 0.3)\n",
    "        input_playlist = clean_songs[:splitter]\n",
    "        eval_data = clean_songs[splitter:]\n",
    "        eval_dict[playlist] = [input_playlist, eval_data]\n",
    "        count += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to get playlsit of given length\n",
    "def n_length_playlists(n):\n",
    "    final_eval = []\n",
    "    for playlist, song_list in eval_dict.items():\n",
    "        if len(song_list[0]) + len(song_list[1]) == n:\n",
    "            final_eval.append(song_list)\n",
    "    random.shuffle(final_eval)\n",
    "    return final_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[\"I'm Shipping Up To Boston\", 'Sympathy For The Devil', 'Uprising', 'Lightning Crashes', \"'Till I Collapse\", 'Gimme Shelter', 'Smells Like Teen Spirit'], ['In The Air Tonight - 2015 Remastered', 'Born to Run', 'Bad Moon Rising']], [['I Feel It Coming', 'All I Know', 'Six Feet Under', 'Starboy', 'The Only One', 'Needed Me', 'Secrets'], ['LUV', 'Touch It', 'Die For You']]]\n"
     ]
    }
   ],
   "source": [
    "o = n_length_playlists(10)\n",
    "print(o[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "import math \n",
    "\n",
    "## recommender that returns list of track names recommended\n",
    "def eval_recommender(playlist, embedding_dict, n):\n",
    "    ## initializing object\n",
    "    sr = SongRecommender(playlist, embedding_dict)\n",
    "    ## finding avg vector\n",
    "    user_vec = sr.finding_avg_vector()\n",
    "    ## getting most similar songs, with artist and similarity score\n",
    "    recommended_songs = sr.n_most_similar(user_vec, n)\n",
    "    y_pred = []\n",
    "    ## extracting only the song name\n",
    "    for val in recommended_songs:\n",
    "        y_pred.append(val[0])\n",
    "    return y_pred\n",
    "\n",
    "## getting average accuracy score of a given word embedding model of 69000 playlists\n",
    "def evaluate_model(embedding_dict, final_eval):\n",
    "    print(\"Starting to evaluate\")\n",
    "    cs_sum = 0\n",
    "    i = 0\n",
    "    for tupl in final_eval:\n",
    "        ## extracting \"train and test\" data\n",
    "        input_playlist = tupl[0]\n",
    "        real = tupl[1]\n",
    "        ## getting avg vector of ytrue\n",
    "        sr1 = SongRecommender(real, embedding_dict)\n",
    "        y_true = sr1.finding_avg_vector()\n",
    "        ## getting recommended songs in list format\n",
    "        recommended = eval_recommender(input_playlist, embedding_dict, len(real))\n",
    "        \n",
    "        ##getting avg vector of recommended songs\n",
    "        sr2 = SongRecommender(recommended, embedding_dict)\n",
    "        y_pred = sr2.finding_avg_vector()\n",
    "        \n",
    "        ## comparing avg vectors of true and predicted recommended songs\n",
    "        y_true = np.reshape(y_true, (1, y_true.shape[0]))\n",
    "        y_pred = np.reshape(y_pred, (1, y_pred.shape[0]))\n",
    "        \n",
    "        cs = float(cosine_similarity(y_true, y_pred))\n",
    "        print(\"THIS IS COSINE SIMILARTY\", cs)\n",
    "        print(\"THIS IS I\", i)\n",
    "        if i == 10:\n",
    "            cs_avg = cs_sum / i\n",
    "            return cs_avg\n",
    "        i+=1\n",
    "        cs_sum += cs\n",
    "        \n",
    "    js_avg = js_sum / len(eval_dict)\n",
    "    return js_avg\n",
    "\n",
    "def eval_diff_playlist_sizes(sizes, embedding_dict):\n",
    "    scores = []\n",
    "    for size in sizes:\n",
    "        scores.append(evaluate_model(embedding_dict, size))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning TFIDF and Doc2Vec vector dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Audio features to recommendation (increasing performance?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##number of songs not in track_names 140002\n",
    "##number of songs in track_names 2115060"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
